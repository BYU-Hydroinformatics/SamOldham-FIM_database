{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "0fvCFGlTYc3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to fill out all of the fields you can.  \n",
        "\n",
        "*   If you want to save your work click File --> Save As and save your own copy of this notebook.\n",
        "*   All of the necessary files for this demonstration can be downloaded [here](https://byu.box.com/s/le7kyjbf80sfyfijyv1k0ov3go6kjdfe).\n",
        "*   Event Date and Additional Model Notes are optional.\n",
        "*   Include the file extension where relevant (target_db_filename should have the extension .db, csv_name, rating_curve_filename, and containing_huc8_filename should have the extention .csv).\n",
        "*  Downstream_reach_id and containing_huc_8 can be found from the linked colab notebook at this link ([NWM Data Finder](https://colab.research.google.com/drive/18CAyUd4ffUoNWDLvbaBVTw0XVSQrHNWI?usp=sharing)) or simply input in the right format if they are already known.\n",
        "* Make sure to download any data you want to save from the Google Colab file explorer when you are done, otherwise it will get deleted when the runtime expires\n"
      ],
      "metadata": {
        "id": "cXUiLcdmU0Rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Data to the Database"
      ],
      "metadata": {
        "id": "_DFKnf09kf2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import sqlite3"
      ],
      "metadata": {
        "id": "poyHP84VCMMN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step requires you to upload a correctly formatted csv of your input data if you choose that option, otherwise a csv file will be generated to save your input data.\n",
        "*  If you choose the Existing Database option under DatabaseType you can upload an existing database with the same format as this database that you wish to add data to."
      ],
      "metadata": {
        "id": "GR11PvpoY2p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title General Input Data\n",
        "# Input to Populate fim_source Table\n",
        "# @markdown Change the input_type parameter to CSV File if you want to populate the fields from a csv file.\n",
        "input_type = 'CSV File' # @param [\"CSV File\", \"Field Input\"]\n",
        "DatabaseType = 'Existing Database' # @param [\"New Database\", \"Existing Database\"]\n",
        "target_db_filename = 'MississippiFIMDatabase.db' # @param {type:\"string\"}\n",
        "# @markdown Ignore this field if your input_type is not CSV File.\n",
        "csv_name = 'FIM_input_data.csv' # @param {type:\"string\"}\n",
        "# @markdown Fill out the following fields if you chose the Field Input Option:\n",
        "FIMSourceName = 'MSTallahatchieRiver_SRH_2D1' # @param {type:\"string\"}\n",
        "CoordinateReference = 'EPSG:4326' # @param {type:\"string\"}\n",
        "EntityName = 'Aquaveo LLC' # @param {type:\"string\"}\n",
        "EntityContactEmail = 'samueljo@byu.edu' # @param {type:\"string\"}\n",
        "VersionNumber = '1.0' # @param {type:\"string\"}\n",
        "YearCreated = 2023 # @param {type:\"integer\"}\n",
        "EventDate = \"\" # @param {type:\"date\"}\n",
        "AdditionalModelNotes = 'This model was created by the FHWA.' # @param {type:\"string\"}\n",
        "# Input to Populate model_type Table\n",
        "Software = 'SRH-2D' # @param [\"HEC RAS-1D\", \"HEC RAS-2D\", \"HEC RAS-1D/2D Combo\", \"SRH-2D\", \"FIER\", \"AutoRoute\", \"HAND\", \"TRITON\", \"Satellite Observations\", \"Surveyed Flood Extents\", \"Other\"]\n",
        "\n",
        "blue = '\\033[94m'\n",
        "italics = '\\033[3m'\n",
        "end = '\\033[0m'\n",
        "\n",
        "if input_type == 'CSV File':\n",
        "  if not os.path.isfile(csv_name):\n",
        "    raise ValueError('CSV input file not found. Make sure to upload your csv file to the files tab or choose the Field Input option for input_type')\n",
        "  input_data_df = pd.read_csv(csv_name)\n",
        "else:\n",
        "  input_data = {\n",
        "  \"FIMSourceName\": FIMSourceName,\n",
        "  \"CoordinateReference\": CoordinateReference,\n",
        "  \"EntityName\": EntityName,\n",
        "  \"EntityContactEmail\": EntityContactEmail,\n",
        "  \"VersionNumber\": VersionNumber,\n",
        "  \"YearCreated\": YearCreated,\n",
        "  \"EventDate\": EventDate,\n",
        "  \"AdditionalModelNotes\": AdditionalModelNotes,\n",
        "  \"Software\": Software\n",
        "  }\n",
        "  input_data_df = pd.DataFrame(input_data, index = [0])\n",
        "  input_data_df.to_csv('FIM_input_data.csv', index=False)\n",
        "  print('Inputs saved as '+ blue + italics + 'FIM_input_data.csv'+ end +'.')\n",
        "\n",
        "database_name = target_db_filename\n",
        "\n",
        "\n",
        "def check_database_exists(filename):\n",
        "  if os.path.isfile(filename) and DatabaseType != 'Existing Database':\n",
        "    raise ValueError('Database file already exists, please change DatabaseType to Existing Database')\n",
        "  if not os.path.isfile(filename) and DatabaseType != 'New Database':\n",
        "    raise ValueError('Database file not found, please upload a database file and change the database name to match the file name or select the New Database option for DatabaseType')\n",
        "\n",
        "check_database_exists(target_db_filename)\n",
        "\n",
        "# Create database structure\n",
        "conn = sqlite3.connect(database_name)\n",
        "dbml_script = \"\"\"\n",
        "Table fim_source {\n",
        "  FIMSourceID integer [pk, increment]\n",
        "  FIMSourceName text\n",
        "  CoordinateReference text\n",
        "  PrimaryRatingCurveID integer\n",
        "  ResponsibleEntityID integer\n",
        "  ModelTypeID integer\n",
        "  VersionNumber tinytext\n",
        "  YearCreated smallint\n",
        "  EventDate datetime\n",
        "  AdditionalModelNotes text\n",
        "}\n",
        "\n",
        "Table responsible_entities {\n",
        "  ResponsibleEntityID integer [pk, increment]\n",
        "  EntityName text\n",
        "  EntityContactEmail text\n",
        "}\n",
        "\n",
        "Table feature_ids_fim_source {\n",
        "  FeatureID integer\n",
        "  RatingCurveID integer\n",
        "  FimSourceID integer\n",
        "  ReferenceNetwork text\n",
        "}\n",
        "\n",
        "Table model_type {\n",
        "  ModelTypeID smallint [pk, increment]\n",
        "  Software text\n",
        "}\n",
        "\n",
        "Table flows {\n",
        "  RatingCurveID integer\n",
        "  Flow float(2)\n",
        "  Depth float(2)\n",
        "  ReturnPeriod smallint\n",
        "  FloodExtentVectorID integer\n",
        "  DepthRasterID integer\n",
        "  WSERasterID integer\n",
        "  VelocityRasterID integer\n",
        "  BoundaryVectorID integer\n",
        "}\n",
        "\n",
        "Table files {\n",
        "  FileID integer [pk, increment]\n",
        "  FileName text\n",
        "}\n",
        "\n",
        "Table rating_curves {\n",
        "  RatingCurveID integer [pk, increment]\n",
        "  RatingCurve text\n",
        "  FlowUnit tinytext\n",
        "  DepthUnit tinytext\n",
        "}\n",
        "\n",
        "Table fim_source_huc8 {\n",
        "  FIMSourceID integer\n",
        "  HUCNumber integer\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def dbml_to_sqlite(dbml_script, database_name):\n",
        "    table_statements = [stmt.strip() for stmt in dbml_script.split('Table') if stmt.strip()]\n",
        "    conn = sqlite3.connect(database_name)\n",
        "    cursor = conn.cursor()\n",
        "    for table_statement in table_statements:\n",
        "        table_name, columns = table_statement.split('{')[0].strip(), table_statement.split('{')[1].split('}')[0].strip()\n",
        "        columns = \", \".join([col.strip() for col in columns.split('\\n') if col.strip()])\n",
        "        cursor.execute(f\"CREATE TABLE {table_name} ({columns})\")\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    print('New database ' + blue + italics + f'{database_name}' + end + ' successfully created.')\n",
        "\n",
        "if DatabaseType == 'New Database':\n",
        "  dbml_to_sqlite(dbml_script, database_name)\n",
        "elif DatabaseType == 'Existing Database':\n",
        "  print(f\"Database Name: '{target_db_filename}\")\n",
        "\n",
        "def check_for_duplicate_fim_sources(fim_sources):\n",
        "  fim_sources = input_data_df['FIMSourceName'].to_list()\n",
        "  duplicates = []\n",
        "  seen = set()\n",
        "  existing_fim_sources = set(pd.read_sql_query(\"SELECT FIMSourceName FROM fim_source\", conn)[\"FIMSourceName\"].tolist())\n",
        "  for fim_source in fim_sources:\n",
        "    if fim_source in seen or fim_source in existing_fim_sources:\n",
        "      duplicates.append(fim_source)\n",
        "    else:\n",
        "      seen.add(fim_source)\n",
        "  return duplicates\n",
        "\n",
        "duplicates = check_for_duplicate_fim_sources(input_data_df)\n",
        "if duplicates:\n",
        "  raise ValueError(f'Duplicate FIM Source Names found: {\", \".join(duplicates)}')\n",
        "\n",
        "# Code to add data to the database\n",
        "def insert_data_into_table(conn, table_name, data_frame):\n",
        "    data_frame.to_sql(table_name, conn, if_exists='append', index=False)\n",
        "\n",
        "# Populate model_type table if necessary\n",
        "model_type = {\n",
        "    'ModelTypeID': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'Software': [\"HEC RAS-1D\", \"HEC RAS-2D\", \"HEC RAS-1D/2D Combo\", \"SRH-2D\", \"FIER\", \"AutoRoute\", \"HAND\", \"TRITON\", \"Satellite Observations\", \"Surveyed Flood Extents\", \"Other\"]\n",
        "}\n",
        "model_type_number = pd.read_sql_query(\"\"\"SELECT COUNT(*) FROM model_type\"\"\",conn)\n",
        "if model_type_number.iloc[0,0] == 0:\n",
        "  model_type_df = pd.DataFrame(model_type)\n",
        "  insert_data_into_table(conn, \"model_type\", model_type_df)\n",
        "\n",
        "# Add data to fim_source_table\n",
        "num_fim_sources_input = len(input_data_df)\n",
        "fim_source_numbers = list(range(0, num_fim_sources_input))\n",
        "start_fim_source_number = pd.read_sql_query(\"\"\"SELECT COUNT(*) FROM fim_source\"\"\",conn)\n",
        "start_fim_source_number = start_fim_source_number.iloc[0,0]\n",
        "fim_source_numbers = [num + start_fim_source_number for num in fim_source_numbers]\n",
        "print(f\"New FIM Source IDs assigned to Input FIM Sources: {fim_source_numbers}\")\n",
        "# Get ResponsibleEntityID\n",
        "starting_entity_number = pd.read_sql_query(\"\"\"SELECT COUNT(*) FROM responsible_entities\"\"\", conn).iloc[0,0]\n",
        "for entity_name in input_data_df['EntityName'].unique():\n",
        "  # Check each unique entity name to see if it is in the database, and if it isn't then add it\n",
        "  responsible_entity_check_existing = pd.read_sql_query(\n",
        "      f\"SELECT ResponsibleEntityID FROM responsible_entities WHERE EntityName LIKE '{entity_name}'\", conn)\n",
        "  if responsible_entity_check_existing.empty:\n",
        "    responsible_entity_num = starting_entity_number\n",
        "    input_data_df.loc[input_data_df['EntityName'] == entity_name, 'ResponsibleEntityID'] = responsible_entity_num\n",
        "    starting_entity_number += 1\n",
        "  else:\n",
        "    responsible_entity_num = responsible_entity_check_existing.iloc[0,0]\n",
        "    input_data_df.loc[input_data_df['EntityName'] == entity_name, 'ResponsibleEntityID'] = responsible_entity_num\n",
        "# Get ModelTypeID\n",
        "model_type_data = input_data_df['Software']\n",
        "for Software in input_data_df['Software'].unique():\n",
        "  model_type_check_existing = pd.read_sql_query(f\"SELECT ModelTypeID FROM model_type WHERE Software LIKE '{Software}'\",conn)\n",
        "  model_type_num = model_type_check_existing.iloc[0,0]\n",
        "  input_data_df.loc[input_data_df['Software'] == Software, 'ModelTypeID'] = model_type_num\n",
        "input_data_df = input_data_df.astype({'ModelTypeID': int,'ResponsibleEntityID': int})\n",
        "input_data_df.loc[:, 'FIMSourceID'] = fim_source_numbers\n",
        "formats = ['%m/%d/%Y', '%m/%d/%y']\n",
        "for fmt in formats:\n",
        "  try:\n",
        "    input_data_df['EventDate'] = pd.to_datetime(input_data_df['EventDate'], format=fmt)\n",
        "    break\n",
        "  except ValueError:\n",
        "    pass\n",
        "fim_source_df = input_data_df[[\"FIMSourceID\",\"FIMSourceName\",\"CoordinateReference\",\"ResponsibleEntityID\",\"ModelTypeID\",\"VersionNumber\",\"YearCreated\",\"EventDate\",\"AdditionalModelNotes\"]]\n",
        "insert_data_into_table(conn, \"fim_source\", fim_source_df)\n",
        "\n",
        "# Add data to responsible_entities table\n",
        "responsible_entities_df = input_data_df[['ResponsibleEntityID','EntityName', 'EntityContactEmail']]\n",
        "unique_entity_ids = responsible_entities_df.groupby('ResponsibleEntityID').head(1)\n",
        "existing_ids_df = pd.read_sql_query(\"SELECT ResponsibleEntityID FROM responsible_entities\", conn)\n",
        "existing_ids = set(existing_ids_df['ResponsibleEntityID'].tolist())\n",
        "data_to_insert = unique_entity_ids[~unique_entity_ids['ResponsibleEntityID'].isin(existing_ids)]\n",
        "insert_data_into_table(conn, \"responsible_entities\", data_to_insert)\n",
        "\n",
        "if num_fim_sources_input == 1:\n",
        "  print(f'{num_fim_sources_input} FIM Source successfully added to the database')\n",
        "else:\n",
        "  print(f'{num_fim_sources_input} FIM Sources successfully added to the database')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "ZJpX0fX8tQLv",
        "outputId": "3627ae9a-684b-4c52-d645-2cbde6e6e287"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs saved as \u001b[94m\u001b[3mFIM_input_data.csv\u001b[0m.\n",
            "Database Name: 'MississippiFIMDatabase.db\n",
            "New FIM Source IDs assigned to Input FIM Sources: [1]\n",
            "1 FIM Source successfully added to the database\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to view the fim_source table\n",
        "pd.read_sql_query(\"\"\"SELECT * from fim_source\"\"\",conn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "cellView": "form",
        "id": "CxipwAXScW7R",
        "outputId": "92b6251f-a461-43a1-b557-f5ff7ef2c21e"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   FIMSourceID                FIMSourceName CoordinateReference  \\\n",
              "0            0   MSTallahatchieRiver_SRH_2D           EPSG:4326   \n",
              "1            1  MSTallahatchieRiver_SRH_2D1           EPSG:4326   \n",
              "\n",
              "  PrimaryRatingCurveID  ResponsibleEntityID  ModelTypeID VersionNumber  \\\n",
              "0                 None                    0            3           1.0   \n",
              "1                 None                    0            3           1.0   \n",
              "\n",
              "   YearCreated EventDate                 AdditionalModelNotes  \n",
              "0         2023      None  This model was created by the FHWA.  \n",
              "1         2023      None  This model was created by the FHWA.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6657bddf-3d08-4bbd-b1be-efee50382409\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FIMSourceID</th>\n",
              "      <th>FIMSourceName</th>\n",
              "      <th>CoordinateReference</th>\n",
              "      <th>PrimaryRatingCurveID</th>\n",
              "      <th>ResponsibleEntityID</th>\n",
              "      <th>ModelTypeID</th>\n",
              "      <th>VersionNumber</th>\n",
              "      <th>YearCreated</th>\n",
              "      <th>EventDate</th>\n",
              "      <th>AdditionalModelNotes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>MSTallahatchieRiver_SRH_2D</td>\n",
              "      <td>EPSG:4326</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2023</td>\n",
              "      <td>None</td>\n",
              "      <td>This model was created by the FHWA.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>MSTallahatchieRiver_SRH_2D1</td>\n",
              "      <td>EPSG:4326</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2023</td>\n",
              "      <td>None</td>\n",
              "      <td>This model was created by the FHWA.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6657bddf-3d08-4bbd-b1be-efee50382409')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6657bddf-3d08-4bbd-b1be-efee50382409 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6657bddf-3d08-4bbd-b1be-efee50382409');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6a922d7b-0595-4ccf-937c-0d00ce549b50\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6a922d7b-0595-4ccf-937c-0d00ce549b50')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6a922d7b-0595-4ccf-937c-0d00ce549b50 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Add HUC8 Data\n",
        "# @markdown You can get the HUC8 data from the colab notebook linked in the introduction if you don't already have it.\n",
        "# @markdown Follow the template format for the containing huc8 csv file to ensure correct upload\n",
        "containing_huc8_filename = 'containing_huc8s.csv' #@param {type: \"string\"}\n",
        "target_FimSourceID = 1 # @param {type:\"integer\"}\n",
        "# Add data to the fim_source_huc8 table\n",
        "model_huc8_list = pd.read_csv(containing_huc8_filename)['HUC8'].to_list()\n",
        "fim_source_huc8_dict = {'FIMSourceID': [], 'HUCNumber': []}\n",
        "for huc8 in model_huc8_list:\n",
        "  fim_source_huc8_dict['FIMSourceID'].append(target_FimSourceID)\n",
        "  fim_source_huc8_dict['HUCNumber'].append(huc8)\n",
        "fim_source_huc8_df = pd.DataFrame(fim_source_huc8_dict)\n",
        "existing_huc8_data = pd.read_sql_query(\"SELECT HUCNumber, FIMSourceID FROM fim_source_huc8\", conn)\n",
        "merged_df = fim_source_huc8_df.merge(existing_huc8_data, how='inner', on=['HUCNumber', 'FIMSourceID'])\n",
        "blue = '\\033[94m'\n",
        "italics = '\\033[3m'\n",
        "if not merged_df.empty:\n",
        "  fim_source_huc8_df = fim_source_huc8_df[~fim_source_huc8_df['HUCNumber'].isin(merged_df['HUCNumber'])]\n",
        "  if not fim_source_huc8_df.empty:\n",
        "    insert_data_into_table(conn, \"fim_source_huc8\", fim_source_huc8_df)\n",
        "    print('HUC8 data successfully added to FIM Source with ' + blue + italics + f'FIMSourceID {target_FimSourceID}')\n",
        "  if fim_source_huc8_df.empty:\n",
        "    print('HUC8 number is already associated with FIM Source')\n",
        "else:\n",
        "  insert_data_into_table(conn, \"fim_source_huc8\", fim_source_huc8_df)\n",
        "  print('HUC8 data successfully added to FIM Source with ' + blue + italics + f'FIMSourceID {target_FimSourceID}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "5YzTvLjkOIQJ",
        "outputId": "4beef423-670b-4346-d483-f1ad3a8592cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HUC8 data successfully added to FIM Source with \u001b[94m\u001b[3mFIMSourceID 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to view the fim_source_huc8 table\n",
        "pd.read_sql_query(\"\"\"SELECT * from fim_source_huc8 ORDER BY FIMSourceID\"\"\",conn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "-MalQyR7sMmA",
        "outputId": "3957b3fc-9a4d-4952-cd19-780d9daf3dce",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   FIMSourceID  HUCNumber\n",
              "0            0    8030201\n",
              "1            1    8030201"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c6584c26-89eb-4c09-83c2-a70ce32d20b8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FIMSourceID</th>\n",
              "      <th>HUCNumber</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>8030201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8030201</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6584c26-89eb-4c09-83c2-a70ce32d20b8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c6584c26-89eb-4c09-83c2-a70ce32d20b8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c6584c26-89eb-4c09-83c2-a70ce32d20b8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1788b665-d154-4b49-8e04-b2bce877b360\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1788b665-d154-4b49-8e04-b2bce877b360')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1788b665-d154-4b49-8e04-b2bce877b360 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"FIMSourceID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HUCNumber\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 8030201,\n        \"max\": 8030201,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          8030201\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rating Curve Data"
      ],
      "metadata": {
        "id": "mUmN9j_IHXhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step requires you to upload a rating curve file and a feature_ids file, both of which should be in csv format."
      ],
      "metadata": {
        "id": "7pxi1Pw1ZOSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Rating Curve Data Tool\n",
        "# @markdown Select the appropriate action:\n",
        "rating_curve_action = \"Add New Rating Curve\" # @param [\"Add New Rating Curve\",\"Replace Rating Curve\",\"Delete Rating Curve\"]\n",
        "\n",
        "# @markdown Follow the template format for the rating curve file to ensure correct upload.\n",
        "# @markdown Choose \"Yes\" on the primary rating curve option if this rating curve is for the furthest downstream reach_id of your model.\n",
        "primary_rating_curve = 'Yes' # @param [\"Yes\", \"No\"]\n",
        "rating_curve_filename = 'MS_SRH2DFlows.csv' # @param {type:\"string\"}\n",
        "target_db_filename = 'MississippiFIMDatabase.db' # @param {type:\"string\"}\n",
        "target_FimSourceID = 0 # @param {type:\"integer\"}\n",
        "FlowUnit = 'cfs' # @param [\"cfs\", \"cms\"]\n",
        "DepthUnit = 'ft' # @param [\"ft\", \"m\"]\n",
        "NWMReferenceNetworkVersion = '3.0' # @param [\"1.2\", \"2.0\",\"2.1\",\"3.0\"]\n",
        "\n",
        "# @markdown You can get a feature_id file from the colab notebook linked at the top of this page if you don't already have it.\n",
        "feature_id_filename = 'feature_ids (HAND).csv' # @param {type:\"string\"}\n",
        "\n",
        "# @markdown Choose yes on seperate_files if your rating curve is associated with files not already referenced in the database.\n",
        "# @markdown Choose no if your rating curve references files already connected to another rating curve, then type in the RatingCurveID for that rating curve as source_RatingCurveID.\n",
        "seperate_files = 'Yes' # @param [\"Yes\", \"No\"]\n",
        "source_RatingCurveID = 0 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown Fill in this parameter if replacing or deleting a rating curve with the rating curve ID to be replaced.\n",
        "target_RatingCurveID = 0 # @param {type:\"integer\"}\n",
        "\n",
        "conn = sqlite3.connect(target_db_filename)\n",
        "cur = conn.cursor()\n",
        "if rating_curve_action == 'Add New Rating Curve':\n",
        "  # Attempt to read in files early to prevent partial additions to the database\n",
        "  flows_df = pd.read_csv(rating_curve_filename)\n",
        "  feature_ids = pd.read_csv(feature_id_filename)['feature_id'].to_list()\n",
        "\n",
        "  def insert_data_into_table(conn, table_name, data_frame):\n",
        "      data_frame.to_sql(table_name, conn, if_exists='append', index=False)\n",
        "\n",
        "  # Get RatingCurveID\n",
        "  rating_curve_check = pd.read_sql_query(\"\"\"SELECT COUNT(*) FROM rating_curves\"\"\",conn).iloc[0,0]\n",
        "  if rating_curve_check == 0:\n",
        "    rating_curve_number = 0\n",
        "  else:\n",
        "    rating_curve_number = pd.read_sql_query(\"\"\"SELECT MAX(RatingCurveID) FROM rating_curves\"\"\",conn).iloc[0,0] + 1\n",
        "  # Add RatingCurveID to the fim_source table\n",
        "  if primary_rating_curve == 'Yes':\n",
        "    query = f\"UPDATE fim_source SET PrimaryRatingCurveID = {rating_curve_number} WHERE FimSourceID = '{target_FimSourceID}'\"\n",
        "    cur.execute(query)\n",
        "  # Add data to rating_curves table\n",
        "  rating_curves_df = pd.DataFrame([{'RatingCurveID':rating_curve_number,\"RatingCurve\":rating_curve_filename,'FlowUnit':FlowUnit,'DepthUnit':DepthUnit}])\n",
        "  insert_data_into_table(conn, \"rating_curves\", rating_curves_df)\n",
        "\n",
        "  # Add data to the flows table\n",
        "  if seperate_files == 'Yes': # This is if you want to make new files for each new file\n",
        "    flows_df['RatingCurveID'] = rating_curve_number\n",
        "    vector_file_id = pd.read_sql_query(\"\"\"SELECT COUNT(*) FROM files\"\"\",conn).iloc[0,0]\n",
        "    for idx, row in flows_df.iterrows():\n",
        "      flows_df.at[idx, 'FloodExtentVectorID'] = vector_file_id\n",
        "      vector_file_id += 1\n",
        "      if not pd.isna(row['DepthRaster']):\n",
        "        depth_file_id = vector_file_id\n",
        "        flows_df.at[idx, 'DepthRasterID'] = depth_file_id\n",
        "        vector_file_id += 1\n",
        "      if not pd.isna(row['WSERaster']):\n",
        "        if not pd.isna(row['DepthRaster']):\n",
        "          wse_file_id = depth_file_id + 1\n",
        "        else:\n",
        "          wse_file_id = vector_file_id\n",
        "        flows_df.at[idx, 'WSERasterID'] = wse_file_id\n",
        "        vector_file_id += 1\n",
        "      if not pd.isna(row['VelocityRaster']):\n",
        "        if not pd.isna(row['DepthRaster']):\n",
        "          velocity_file_id = depth_file_id + 1\n",
        "        if not pd.isna(row['WSERaster']):\n",
        "          velocity_file_id = wse_file_id + 1\n",
        "        else:\n",
        "          velocity_file_id = vector_file_id\n",
        "        flows_df.at[idx, 'VelocityRasterID'] = velocity_file_id\n",
        "        vector_file_id += 1\n",
        "      if not pd.isna(row['BoundaryVector']):\n",
        "        if not pd.isna(row['DepthRaster']):\n",
        "          boundary_file_id = depth_file_id + 1\n",
        "        if not pd.isna(row['WSERaster']):\n",
        "          boundary_file_id = wse_file_id + 1\n",
        "        if not pd.isna(row['VelocityRaster']):\n",
        "          boundary_file_id = velocity_file_id + 1\n",
        "        else:\n",
        "          boundary_file_id = vector_file_id\n",
        "        flows_df.at[idx, 'BoundaryVectorID'] = boundary_file_id\n",
        "        vector_file_id = boundary_file_id + 1\n",
        "\n",
        "    for col in ['DepthRasterID', 'WSERasterID','VelocityRasterID','BoundaryVectorID']:\n",
        "      if col not in flows_df.columns:\n",
        "        flows_df[col] = np.NaN\n",
        "    flows_data_df = flows_df\n",
        "\n",
        "    columns_to_check = ['DepthRasterID', 'WSERasterID', 'VelocityRasterID', 'BoundaryVectorID']\n",
        "    columns_to_include = ['RatingCurveID', 'Flow', 'Depth', 'ReturnPeriod', 'FloodExtentVectorID']\n",
        "    # Check if any of the columns exist and include them in the final DataFrame\n",
        "    existing_columns = [col for col in columns_to_check if col in flows_df.columns]\n",
        "    flows_df = flows_df[columns_to_include + existing_columns]\n",
        "    insert_data_into_table(conn, \"flows\", flows_df)\n",
        "    # Add data to the files table\n",
        "    files_dict = {'FileID': [], 'FileName': []}\n",
        "    for idx, row in flows_data_df.iterrows():\n",
        "      vector_extent = row['VectorExtent']\n",
        "      depth_raster = row['DepthRaster']\n",
        "      wse_raster = row['WSERaster']\n",
        "      vel_raster = row['VelocityRaster']\n",
        "      boundary_vector = row['BoundaryVector']\n",
        "      vector_id = row['FloodExtentVectorID']\n",
        "      depth_id = row['DepthRasterID']\n",
        "      wse_id = row['WSERasterID']\n",
        "      vel_id = row['VelocityRasterID']\n",
        "      boundary_id = row['BoundaryVectorID']\n",
        "      files_dict['FileID'].append(vector_id)\n",
        "      files_dict['FileName'].append(vector_extent)\n",
        "      if pd.notna(depth_id):\n",
        "        files_dict['FileID'].append(depth_id)\n",
        "        files_dict['FileName'].append(depth_raster)\n",
        "      if pd.notna(wse_id):\n",
        "        files_dict['FileID'].append(wse_id)\n",
        "        files_dict['FileName'].append(wse_raster)\n",
        "      if pd.notna(vel_id):\n",
        "        files_dict['FileID'].append(vel_id)\n",
        "        files_dict['FileName'].append(vel_raster)\n",
        "      if pd.notna(boundary_id):\n",
        "        files_dict['FileID'].append(boundary_id)\n",
        "        files_dict['FileName'].append(boundary_vector)\n",
        "    files_df = pd.DataFrame(files_dict)\n",
        "    insert_data_into_table(conn, \"files\", files_df)\n",
        "  else: #This is if you want to associate a rating curve with existing files\n",
        "    old_curve = pd.read_sql_query(f'SELECT * from flows WHERE RatingCurveID = {source_RatingCurveID}',conn)\n",
        "    new_curve = pd.read_csv(rating_curve_filename)\n",
        "    new_flow_count = len(rating_curve_filename)\n",
        "    new_curve['FloodExtentVectorID'] = old_curve['FloodExtentVectorID']\n",
        "    new_curve['DepthRasterID'] = old_curve['DepthRasterID']\n",
        "    new_curve['WSERasterID'] = old_curve['WSERasterID']\n",
        "    new_curve['VelocityRasterID'] = old_curve['VelocityRasterID']\n",
        "    new_curve['BoundaryVectorID'] = old_curve['BoundaryVectorID']\n",
        "    new_curve = new_curve[['Flow','Depth','ReturnPeriod','FloodExtentVectorID','DepthRasterID','WSERasterID','VelocityRasterID','BoundaryVectorID']]\n",
        "    insert_data_into_table(conn, \"flows\", new_curve)\n",
        "\n",
        "  # Add data to the feature_ids_fim_source table\n",
        "  feature_ids_dict = {'FeatureID': [], 'RatingCurveID': [], 'FIMSourceID':[], 'ReferenceNetwork':[]}\n",
        "  for feature_id in feature_ids:\n",
        "    feature_ids_dict['FeatureID'].append(feature_id)\n",
        "    feature_ids_dict['RatingCurveID'].append(rating_curve_number)\n",
        "    feature_ids_dict['FIMSourceID'].append(target_FimSourceID)\n",
        "    feature_ids_dict['ReferenceNetwork'].append(NWMReferenceNetworkVersion)\n",
        "  feature_ids_fim_source_df = pd.DataFrame(feature_ids_dict)\n",
        "  insert_data_into_table(conn, \"feature_ids_fim_source\", feature_ids_fim_source_df)\n",
        "\n",
        "  blue = '\\033[94m'\n",
        "  italics = '\\033[3m'\n",
        "  end = '\\033[0m'\n",
        "  print('Rating Curve ' + blue + italics + f'{rating_curve_filename}' + end + f' successfully added to FIM Source with FIMSourceID {target_FimSourceID}')\n",
        "\n",
        "elif rating_curve_action == 'Replace Rating Curve':\n",
        "  # Replace Rating Curve Data with Another Rating Curve\n",
        "  # Attempt to read in files early to prevent partial additions to database\n",
        "  flows_df = pd.read_csv(rating_curve_filename)\n",
        "  feature_ids = pd.read_csv(feature_id_filename)['feature_id'].to_list()\n",
        "\n",
        "  def insert_data_into_table(conn, table_name, data_frame):\n",
        "      data_frame.to_sql(table_name, conn, if_exists='append', index=False)\n",
        "  def insert_data_into_table_replace(conn, table_name, data_frame):\n",
        "      data_frame.to_sql(table_name, conn, if_exists='replace', index=False)\n",
        "\n",
        "  #delete_query = \"DELETE FROM rating_curves WHERE RatingCurveID = ?\"\n",
        "  #cur.execute(delete_query, (target_RatingCurveID,))\n",
        "\n",
        "  # Find each FileID in the files table that is associated with the a flow associated with Rating CurveID and delete it\n",
        "  delete_query1 = \"\"\" DELETE FROM files WHERE FileID IN (SELECT FloodExtentVectorID  FROM flows  WHERE RatingCurveID = ?) \"\"\"\n",
        "  delete_query2 = \"\"\" DELETE FROM files WHERE FileID IN (SELECT DepthRasterID  FROM flows  WHERE RatingCurveID = ?) \"\"\"\n",
        "  delete_query3 = \"\"\" DELETE FROM files WHERE FileID IN (SELECT WSERasterID  FROM flows  WHERE RatingCurveID = ?) \"\"\"\n",
        "  delete_query4 = \"\"\" DELETE FROM files WHERE FileID IN (SELECT VelocityRasterID  FROM flows  WHERE RatingCurveID = ?) \"\"\"\n",
        "  delete_query5 = \"\"\" DELETE FROM files WHERE FileID IN (SELECT BoundaryVectorID  FROM flows  WHERE RatingCurveID = ?) \"\"\"\n",
        "  cur.execute(delete_query1, (target_RatingCurveID,))\n",
        "  cur.execute(delete_query2, (target_RatingCurveID,))\n",
        "  cur.execute(delete_query3, (target_RatingCurveID,))\n",
        "  cur.execute(delete_query4, (target_RatingCurveID,))\n",
        "  cur.execute(delete_query5, (target_RatingCurveID,))\n",
        "\n",
        "  delete_query = \"DELETE FROM flows WHERE RatingCurveID = ?\"\n",
        "  cur.execute(delete_query, (target_RatingCurveID,))\n",
        "\n",
        "  # Set RatingCurveID\n",
        "  rating_curve_number = target_RatingCurveID\n",
        "  # Add RatingCurveID to the fim_source table\n",
        "  if primary_rating_curve == 'Yes':\n",
        "    query = f\"UPDATE fim_source SET PrimaryRatingCurveID = {rating_curve_number} WHERE FimSourceID = '{target_FimSourceID}'\"\n",
        "    cur.execute(query)\n",
        "  # Add data to rating_curves table\n",
        "  # Update RatingCurve details in rating_curves table\n",
        "  query = f\"UPDATE rating_curves SET RatingCurve = '{rating_curve_filename}', FlowUnit = '{FlowUnit}', DepthUnit = '{DepthUnit}' WHERE RatingCurveID = {rating_curve_number}\"\n",
        "  cur.execute(query)\n",
        "\n",
        "  # Add data to the flows table\n",
        "  flows_df['RatingCurveID'] = rating_curve_number\n",
        "  new_flow_count = len(flows_df)\n",
        "  # Query the files table and see how many there are in the database already\n",
        "  query = \"SELECT COUNT(*) FROM files\"\n",
        "  cur.execute(query)\n",
        "  existing_files_count = cur.fetchone()[0]\n",
        "\n",
        "  # Find each FileID already existing in the files table\n",
        "  query1 = \"\"\"SELECT FloodExtentVectorID FROM flows WHERE RatingCurveID != ?\"\"\"\n",
        "  query2 = \"\"\"SELECT DepthRasterID FROM flows WHERE RatingCurveID != ?\"\"\"\n",
        "  query3 = \"\"\"SELECT WSERasterID FROM flows WHERE RatingCurveID != ?\"\"\"\n",
        "  query4 = \"\"\"SELECT VelocityRasterID FROM flows WHERE RatingCurveID != ?\"\"\"\n",
        "  query5 = \"\"\"SELECT BoundaryVectorID FROM flows WHERE RatingCurveID != ?\"\"\"\n",
        "  flood_extent_vector_data = cur.execute(query1, (target_RatingCurveID,)).fetchall()\n",
        "  flood_extent_vector_ids = [element[0] for element in flood_extent_vector_data]\n",
        "  depth_raster_data = cur.execute(query2, (target_RatingCurveID,)).fetchall()\n",
        "  depth_raster_ids = [element[0] for element in depth_raster_data]\n",
        "  wse_raster_data = cur.execute(query3, (target_RatingCurveID,)).fetchall()\n",
        "  wse_raster_ids = [element[0] for element in wse_raster_data]\n",
        "  vel_raster_data = cur.execute(query4, (target_RatingCurveID,)).fetchall()\n",
        "  vel_raster_ids = [element[0] for element in vel_raster_data]\n",
        "  boundary_vector_data = cur.execute(query5, (target_RatingCurveID,)).fetchall()\n",
        "  boundary_vector_ids = [element[0] for element in boundary_vector_data]\n",
        "\n",
        "  file_ids = flood_extent_vector_ids + depth_raster_ids + wse_raster_ids + vel_raster_ids + boundary_vector_ids\n",
        "  file_ids = [file_id for file_id in file_ids if pd.notna(file_id)]\n",
        "  file_ids = sorted(file_ids, key=lambda x: int(x))\n",
        "  # Determine how many new ids to add\n",
        "  flood_extent_vector_count = flows_df['VectorExtent'].count()\n",
        "  depth_raster_count = flows_df['DepthRaster'].count()\n",
        "  wse_raster_count = flows_df['WSERaster'].count()\n",
        "  vel_raster_count = flows_df['VelocityRaster'].count()\n",
        "  boundary_vector_count = flows_df['BoundaryVector'].count()\n",
        "  new_ids_count = flood_extent_vector_count + depth_raster_count + wse_raster_count + vel_raster_count + boundary_vector_count\n",
        "  if existing_files_count == 0:\n",
        "    new_file_ids = [i for i in range(new_ids_count)]\n",
        "  else:\n",
        "    new_file_ids = [i for i in range(len(file_ids)+new_ids_count)]\n",
        "  new_file_ids = [i for i in new_file_ids if i not in file_ids]\n",
        "  new_file_ids = sorted(new_file_ids, key=lambda x: int(x))\n",
        "  file_id_counter = 0\n",
        "  for idx, row in flows_df.iterrows():\n",
        "    flows_df.at[idx, 'FloodExtentVectorID'] = new_file_ids[file_id_counter]\n",
        "    file_id_counter += 1\n",
        "    if not pd.isna(flows_df.at[idx, 'DepthRaster']):\n",
        "      flows_df.at[idx, 'DepthRasterID'] = new_file_ids[file_id_counter]\n",
        "      file_id_counter += 1\n",
        "    if not pd.isna(flows_df.at[idx, 'WSERaster']):\n",
        "      flows_df.at[idx, 'WSERasterID'] = new_file_ids[file_id_counter]\n",
        "      file_id_counter += 1\n",
        "    if not pd.isna(flows_df.at[idx, 'VelocityRaster']):\n",
        "      flows_df.at[idx, 'VelocityRasterID'] = new_file_ids[file_id_counter]\n",
        "      file_id_counter += 1\n",
        "    if not pd.isna(flows_df.at[idx, 'BoundaryVector']):\n",
        "      flows_df.at[idx, 'BoundaryVectorID'] = new_file_ids[file_id_counter]\n",
        "      file_id_counter += 1\n",
        "\n",
        "  columns_to_check = ['FloodExtentVectorID', 'DepthRasterID', 'WSERasterID', 'VelocityRasterID', 'BoundaryVectorID']\n",
        "  populated_columns = [col for col in columns_to_check if col in flows_df.columns]\n",
        "  flows_df = flows_df.fillna(-99999).astype({col: int for col in populated_columns})\n",
        "  flows_df[populated_columns] = flows_df[populated_columns].replace(-99999, np.nan)\n",
        "\n",
        "  flows_data_df = flows_df\n",
        "  bad_columns = [col for col in columns_to_check if col not in flows_df.columns]\n",
        "  for col in bad_columns:\n",
        "    flows_data_df[col] = np.NaN\n",
        "\n",
        "  columns_list = ['RatingCurveID', 'Flow', 'Depth', 'ReturnPeriod', 'FloodExtentVectorID', 'DepthRasterID', 'WSERasterID', 'VelocityRasterID', 'BoundaryVectorID']\n",
        "  flows_df_columns = [col for col in columns_list if col in flows_df.columns]\n",
        "  flows_df = flows_df[flows_df_columns]\n",
        "  insert_data_into_table(conn, \"flows\", flows_df)\n",
        "  # Add data to the files table\n",
        "  files_dict = {'FileID': [], 'FileName': []}\n",
        "  for idx, row in flows_data_df.iterrows():\n",
        "    vector_extent = row['VectorExtent']\n",
        "    if not pd.isna(row['DepthRaster']):\n",
        "      depth_raster = row['DepthRaster']\n",
        "    if not pd.isna(row['WSERaster']):\n",
        "      wse_raster = row['WSERaster']\n",
        "    if not pd.isna(row['VelocityRaster']):\n",
        "      vel_raster = row['VelocityRaster']\n",
        "    if not pd.isna(row['BoundaryVector']):\n",
        "      boundary_vector = row['BoundaryVector']\n",
        "    vector_id = row['FloodExtentVectorID']\n",
        "    depth_id = row['DepthRasterID']\n",
        "    wse_id = row['WSERasterID']\n",
        "    vel_id = row['VelocityRasterID']\n",
        "    boundary_id = row['BoundaryVectorID']\n",
        "    files_dict['FileID'].append(vector_id)\n",
        "    files_dict['FileName'].append(vector_extent)\n",
        "    if pd.notna(depth_id):\n",
        "      files_dict['FileID'].append(depth_id)\n",
        "      files_dict['FileName'].append(depth_raster)\n",
        "    if pd.notna(wse_id):\n",
        "      files_dict['FileID'].append(wse_id)\n",
        "      files_dict['FileName'].append(wse_raster)\n",
        "    if pd.notna(vel_id):\n",
        "      files_dict['FileID'].append(vel_id)\n",
        "      files_dict['FileName'].append(vel_raster)\n",
        "    if pd.notna(boundary_id):\n",
        "      files_dict['FileID'].append(boundary_id)\n",
        "      files_dict['FileName'].append(boundary_vector)\n",
        "  files_df = pd.DataFrame(files_dict)\n",
        "  insert_data_into_table(conn, \"files\", files_df)\n",
        "\n",
        "  # Add data to the feature_ids_fim_source table\n",
        "  query5 = \"\"\"DELETE FROM feature_ids_fim_source WHERE RatingCurveID = ?\"\"\"\n",
        "  cur.execute(query5,(rating_curve_number,))\n",
        "  feature_ids_dict = {'FeatureID': [], 'RatingCurveID': [], 'FIMSourceID':[], 'ReferenceNetwork':[]}\n",
        "  for feature_id in feature_ids:\n",
        "    feature_ids_dict['FeatureID'].append(feature_id)\n",
        "    feature_ids_dict['RatingCurveID'].append(rating_curve_number)\n",
        "    feature_ids_dict['FIMSourceID'].append(target_FimSourceID)\n",
        "    feature_ids_dict['ReferenceNetwork'].append(NWMReferenceNetworkVersion)\n",
        "  feature_ids_fim_source_df = pd.DataFrame(feature_ids_dict)\n",
        "  insert_data_into_table(conn, \"feature_ids_fim_source\", feature_ids_fim_source_df)\n",
        "\n",
        "  blue = '\\033[94m'\n",
        "  italics = '\\033[3m'\n",
        "  end = '\\033[0m'\n",
        "  print('Rating Curve ' + blue + italics + f'{rating_curve_filename}' + end + f' successfully added to FIM Source with FIMSourceID {target_FimSourceID}, replacing RatingCurve with ' + blue + italics + f'RatingCurveID {target_RatingCurveID}')\n",
        "elif rating_curve_action == 'Delete Rating Curve':\n",
        "  # Delete Rating Curve Data\n",
        "  check = pd.read_sql_query(f\"SELECT COUNT(*) FROM rating_curves WHERE RatingCurveID = {target_RatingCurveID}\",conn).iloc[0,0]\n",
        "  if check == 0:\n",
        "    raise ValueError(f'Rating Curve with RatingCurveID {target_RatingCurveID} does not exist in the database')\n",
        "\n",
        "  delete_query = \"DELETE FROM rating_curves WHERE RatingCurveID = ?\"\n",
        "  cur.execute(delete_query, (target_RatingCurveID,))\n",
        "  # Find each file in the files table that is associated with the rating curve and delete it\n",
        "  delete_query1 = \"\"\" DELETE FROM files WHERE FileID IN (SELECT FloodExtentVectorID  FROM flows  WHERE RatingCurveID = ?) \"\"\"\n",
        "  delete_query2 = \"\"\" DELETE FROM files WHERE FileID IN (SELECT DepthRasterID  FROM flows  WHERE RatingCurveID = ?) \"\"\"\n",
        "  delete_query3 = \"\"\" DELETE FROM files WHERE FileID IN (SELECT WSERasterID  FROM flows  WHERE RatingCurveID = ?) \"\"\"\n",
        "  delete_query4 = \"\"\" DELETE FROM files WHERE FileID IN (SELECT VelocityRasterID  FROM flows  WHERE RatingCurveID = ?) \"\"\"\n",
        "  delete_query5 = \"\"\" DELETE FROM files WHERE FileID IN (SELECT BoundaryVectorID  FROM flows  WHERE RatingCurveID = ?) \"\"\"\n",
        "  cur.execute(delete_query1, (target_RatingCurveID,))\n",
        "  cur.execute(delete_query2, (target_RatingCurveID,))\n",
        "  cur.execute(delete_query3, (target_RatingCurveID,))\n",
        "  cur.execute(delete_query4, (target_RatingCurveID,))\n",
        "  cur.execute(delete_query5, (target_RatingCurveID,))\n",
        "  # Delete entries from reach_ids_fim_source related to the rating curve\n",
        "  query5 = \"\"\"DELETE FROM feature_ids_fim_source WHERE RatingCurveID = ?\"\"\"\n",
        "  cur.execute(query5,(target_RatingCurveID,))\n",
        "  # Delete entries from flows related to the rating curve\n",
        "  delete_query = \"DELETE FROM flows WHERE RatingCurveID = ?\"\n",
        "  cur.execute(delete_query, (target_RatingCurveID,))\n",
        "  blue = '\\033[94m'\n",
        "  italics = '\\033[3m'\n",
        "  end = '\\033[0m'\n",
        "  print('Rating Curve with ' + blue + italics + f'RatingCurveID {target_RatingCurveID}' + end + f' successfully deleted from the database')"
      ],
      "metadata": {
        "id": "UnRoelaibqCy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b0d1af-db84-4fba-b754-1b9ded7f6a6f",
        "cellView": "form"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rating Curve \u001b[94m\u001b[3mMS_SRH2DFlows.csv\u001b[0m successfully added to FIM Source with FIMSourceID 0, replacing RatingCurve with \u001b[94m\u001b[3mRatingCurveID 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to see the rating_curves table\n",
        "pd.read_sql_query(\"\"\"SELECT * from rating_curves ORDER BY RatingCurveID\"\"\",conn)"
      ],
      "metadata": {
        "id": "dzqYZxDNTJg9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An easy way to view the entire database is to download it and then upload it to a sqlite viewer. To do this click on the Google Colab file explorer (file icon) on the left hand side of this webpage, click the three dots by the database file, and click download. Then go to the webpage https://sqliteviewer.app/ and upload your database file to view it."
      ],
      "metadata": {
        "id": "M4dPs4GemnFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# View Additional Database Tables"
      ],
      "metadata": {
        "id": "a_Pj3ruPejLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to view the contents of individual tables you can use the cells below which contain SQL code to view each table not already shown in the cells above."
      ],
      "metadata": {
        "id": "p9i3XDpAnPeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_sql_query(\"\"\"SELECT * from feature_ids_fim_source ORDER BY RatingCurveID\"\"\",conn)"
      ],
      "metadata": {
        "id": "oz7wOm2RiMNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_sql_query(\"\"\"SELECT * from files ORDER BY FileID\"\"\",conn)"
      ],
      "metadata": {
        "id": "jWQBAfsETeKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_sql_query(\"\"\"SELECT * from flows ORDER BY RatingCurveID, Flow DESC\"\"\",conn)"
      ],
      "metadata": {
        "id": "Y5MfROdeTVGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_sql_query(\"\"\"SELECT * from responsible_entities ORDER BY ResponsibleEntityID\"\"\",conn)"
      ],
      "metadata": {
        "id": "5yzu8AbA6EkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload Database and Data to Hydroshare"
      ],
      "metadata": {
        "id": "OeB180YCAunJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to the [HydroShare website](https://www.hydroshare.org/home/) and create an account there which will allow you to upload your data."
      ],
      "metadata": {
        "id": "cNp-7LJiBa8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hsclient -q &> log.log\n",
        "from google.colab import files\n",
        "from hsclient import HydroShare\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import sqlite3"
      ],
      "metadata": {
        "id": "DZSGPqCuBRjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below to sign into HydroShare. Sign in using your HydroShare account credentials."
      ],
      "metadata": {
        "id": "6FAc9lDNBpTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hs = HydroShare()\n",
        "hs.sign_in()"
      ],
      "metadata": {
        "id": "U1ZD27xb_vZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in and run the cell below to create a csv list of the files you need to upload to HydroShare from a database (.db) file.\n",
        "\n",
        "* After running this cell make sure to upload your rating curves and all of your raster and vector files to the file explorer. Before uploading ensure that your shapefiles and rasters are in the correct projection (EPSG: 4326). This will allow them to be visualized through HydroShare.\n",
        "\n",
        "* Make sure to upload all of the files associated with a shapefile, not just the one with the .shp extension."
      ],
      "metadata": {
        "id": "qKMuE04DRbrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the rating_curves.csv file, then make a list of all the values under the VectorExtent, DepthRaster, and WSERaster columns\n",
        "rating_curve_csv = 'MSTallahatchieRiver_SRH2DFlows.csv' #@param {type:\"string\"}\n",
        "db_file = 'FIMDatabase.db' #@param {type:\"string\"}\n",
        "name_file = pd.read_csv(rating_curve_csv)\n",
        "filenames = name_file['VectorExtent'].to_list()\n",
        "filenames += name_file['DepthRaster'].to_list()\n",
        "filenames += name_file['WSERaster'].to_list()\n",
        "filenames += name_file['VelocityRaster'].to_list()\n",
        "filenames += name_file['BoundaryVector'].to_list()\n",
        "filenames = list(set(filenames))\n",
        "filenames.append(db_file)\n",
        "filenames.append(rating_curve_csv)\n",
        "json_filename = os.path.splitext(db_file)[0]+'.json'\n",
        "filenames.append(json_filename)\n",
        "filenames.append(\"README.txt\")\n",
        "filenames = [x for x in filenames if str(x) != 'nan']\n",
        "filenames_df = pd.DataFrame(filenames)\n",
        "filenames_df.columns = ['filename']\n",
        "filenames_df.to_csv('filenames.csv', index=False)\n",
        "print(\"csv file 'filenames.csv' added to the Google Colab file explorer\")\n",
        "\n",
        "# Create a README.txt file\n",
        "with open(\"README.txt\", 'w') as f:\n",
        "    f.write(f\"\"\"The database file is the {db_file} file, and it is also available as a JSON file as {json_filename}.\n",
        "\n",
        "The rating curves are the CSV files in the resource, and the individual rasters and shapefiles are the extent, depth, and water surface elevation files.\n",
        "\n",
        "The file naming system is set up so that it is FIMSourceID-RatingCurveID-type-flow. Type is either Extent (EXT), Depth (DEP), or Water Surface Elevation (WSE).\"\"\")\n",
        "\n",
        "print(\"README.txt file created successfully and added to the Google Colab file explorer\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ykEm2pwaqMGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create filenames.csv containing a list of files to upload\n",
        "#@markdown Choose whether to read filenames from a whole database or from a rating curve\n",
        "read_from = 'Rating Curve' #@param [\"Database\",\"Rating Curve\"]\n",
        "rating_curve_csv = 'MS_SRH2DFlows.csv' #@param {type:\"string\"}\n",
        "db_file = 'FIMDatabase.db' #@param {type:\"string\"}\n",
        "if read_from == 'Rating Curve':\n",
        "  name_file = pd.read_csv(rating_curve_csv)\n",
        "  filenames = name_file['VectorExtent'].to_list()\n",
        "  filenames += name_file['DepthRaster'].to_list()\n",
        "  filenames += name_file['WSERaster'].to_list()\n",
        "  filenames += name_file['VelocityRaster'].to_list()\n",
        "  filenames += name_file['BoundaryVector'].to_list()\n",
        "  filenames = list(set(filenames))\n",
        "  filenames.append(db_file)\n",
        "  filenames.append(rating_curve_csv)\n",
        "  json_filename = os.path.splitext(db_file)[0]+'.json'\n",
        "  filenames.append(json_filename)\n",
        "  filenames.append(\"README.txt\")\n",
        "  filenames = [x for x in filenames if str(x) != 'nan']\n",
        "elif read_from == 'Database':\n",
        "  # Connect to the database\n",
        "  conn = sqlite3.connect(db_file)\n",
        "  cursor = conn.cursor()\n",
        "\n",
        "  # Execute a query to retrieve file paths from the \"files\" table\n",
        "  cursor.execute(\"SELECT filename FROM files\")\n",
        "  filenames = [row[0] for row in cursor.fetchall()]\n",
        "  cursor.execute(\"SELECT RatingCurve FROM rating_curves\")\n",
        "  filenames += [row[0] for row in cursor.fetchall()]\n",
        "  json_filename = os.path.splitext(db_file)[0]+'.json'\n",
        "  # Add other files to the list\n",
        "  filenames += [db_file, json_filename, \"README.txt\"]\n",
        "\n",
        "  # Remove duplicates and NaN values\n",
        "  filenames = list(set(filenames))\n",
        "  filenames = [x for x in filenames if str(x) != 'nan']\n",
        "\n",
        "# Create a DataFrame and save it to a CSV file\n",
        "filenames_df = pd.DataFrame(filenames)\n",
        "filenames_df.columns = ['filename']\n",
        "filenames_df = filenames_df.sort_values(by='filename')\n",
        "filenames_df.to_csv('filenames.csv', index=False)\n",
        "\n",
        "print(\"csv file 'filenames.csv' created and added to the Google Colab file explorer\")\n",
        "\n",
        "# Create a README.txt file\n",
        "with open(\"README.txt\", 'w') as f:\n",
        "    f.write(f\"\"\"The database file is the {db_file} file, and it is also available as a JSON file as {json_filename}.\n",
        "\n",
        "The rating curves are the CSV files in the resource, and the individual rasters and shapefiles are the extent, depth, and water surface elevation files.\n",
        "\n",
        "The file naming system is set up so that it is FIMSourceID-RatingCurveID-type-flow.\n",
        "\n",
        "Type is Extent (EXT), Depth (DEP), Water Surface Elevation (WSE), Velocity (VEL), or Boundary (BND).\"\"\")\n",
        "\n",
        "print(\"README.txt file created and added to the Google Colab file explorer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "PoEr2L2-S2VG",
        "outputId": "c826888b-ce21-44b5-fd6a-49b1a8ba2aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "csv file 'filenames.csv' created and added to the Google Colab file explorer\n",
            "README.txt file created and added to the Google Colab file explorer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the value of the variables in the following fields to include the metadata you wish to include in a new HydroShare resource if you don't already have an existing resource."
      ],
      "metadata": {
        "id": "N-k2C63DVm0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resource_subjects = ['FIM','Flood Inundation Mapping','Flow', 'Discharge', 'River Flow']\n",
        "resource_description = \"\"\"This is a compilation of test FIM maps made from a model we received from USACE (ERDC) of the North Platte River. Please don't use this data except for testing purposes.\"\"\""
      ],
      "metadata": {
        "id": "8gBuDa7hFMFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill out the fields below or ignore them according to the instructions, then run the cell."
      ],
      "metadata": {
        "id": "7J-REwGhtRXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resource_type = 'New' #@param ['New','Existing']\n",
        "# Use this field if you have an existing resource you want to add or update the database in, otherwise leave it blank\n",
        "filenames = 'filenames.csv' #@param {type:\"string\"}\n",
        "db_file = 'NebraskaFIMDatabase.db' #@param {type:\"string\"}\n",
        "#@markdown Fill in resource_name to create a new HydroShare resource if you don't already have one.\n",
        "resource_name = 'BYU FIM Database Nebraska USACE Model' #@param {type:\"string\"}\n",
        "#@markdown Fill in the resource_id if you have an existing HydroShare resource. This id is the end of the resource URL when you view the resource on HydroShare.\n",
        "resource_id = \"d105e214ddd840b09040b6e51246f187\" #@param {type:\"string\"}\n",
        "\n",
        "filenames = pd.read_csv(filenames)\n",
        "# Connect to database\n",
        "conn = sqlite3.connect(db_file)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "def process_dataframe(df):\n",
        "  \"\"\"\n",
        "  Creates a dataframe containing the data necessary to rename the file.\n",
        "  Args:\n",
        "      dataframe (pd.DataFrame): A DataFrame which contains the names of all the files for upload to HydroShare.\n",
        "  Returns:\n",
        "      dataframe (pd.DataFrame): The DataFrame with FIMSourceID, RatingCurveID, and flow for the each filename.\n",
        "  \"\"\"\n",
        "  # Create a dictionary to store FileIDs\n",
        "  file_id_dict = {}\n",
        "  # Efficiently query for FileIDs using placeholders\n",
        "  unique_filenames = df['filename'].unique()\n",
        "  query = f\"SELECT FileName, FileId FROM files WHERE FileName IN ({','.join('?' * len(unique_filenames))})\"\n",
        "  cursor.execute(query, tuple(unique_filenames))\n",
        "  # Store FileIDs in the dictionary\n",
        "  for row in cursor.fetchall():\n",
        "    file_id_dict[row[0]] = row[1]\n",
        "  # Add 'FileID' column based on the dictionary\n",
        "  df['FileID'] = df['filename'].map(file_id_dict)\n",
        "  # Create a dictionary to store associated flows\n",
        "  flow_data_dict = {}\n",
        "  # Define flow queries\n",
        "  flow_queries = [\n",
        "      \"SELECT * FROM flows WHERE FloodExtentVectorID = ?\",\n",
        "      \"SELECT * FROM flows WHERE DepthRasterID = ?\",\n",
        "      \"SELECT * FROM flows WHERE WSERasterID = ?\",\n",
        "      \"SELECT * FROM flows WHERE VelocityRasterID = ?\",\n",
        "      \"SELECT * FROM flows WHERE BoundaryVectorID = ?\"\n",
        "  ]\n",
        "\n",
        "  # Loop through each file and query for associated flows\n",
        "  for file_id in df['FileID'].unique():\n",
        "    flow_data = None\n",
        "    for query in flow_queries:\n",
        "      cursor.execute(query, (file_id,))\n",
        "      result = cursor.fetchone()\n",
        "      if result:\n",
        "        flow_data = result\n",
        "        break  # Stop iterating through flow queries if data is found\n",
        "    flow_data_dict[file_id] = flow_data\n",
        "  # Merge flow data into the dataframe\n",
        "  if flow_data_dict:\n",
        "    flow_df = pd.DataFrame.from_dict(flow_data_dict, orient='index', columns=['RatingCurveID','flow','depth','rp','FloodExtentVectorID','DepthRasterID','WSERasterID'])\n",
        "    joined_df = df.merge(flow_df, how='left', left_on='FileID', right_index=True)\n",
        "  else:\n",
        "    joined_df = df.copy()\n",
        "\n",
        "  rating_curves_exist = pd.read_sql_query(\"\"\"SELECT RatingCurveID, RatingCurve from rating_curves ORDER BY RatingCurveID\"\"\",conn)\n",
        "  joined_df = joined_df.merge(rating_curves_exist, how='left', on='RatingCurveID')\n",
        "  feature_ids_fim_source_exist = pd.read_sql_query(\"\"\"SELECT * from feature_ids_fim_source ORDER BY RatingCurveID\"\"\",conn)\n",
        "  full_data = joined_df.merge(feature_ids_fim_source_exist, how='inner', on='RatingCurveID')\n",
        "  unique_data = []\n",
        "  # Loop through each group defined by RatingCurveID and FIMSourceID\n",
        "  for group_name, group_data in full_data.groupby(['RatingCurveID']):\n",
        "    # Select the first row\n",
        "    first_instance = group_data.iloc[0]\n",
        "    # Append the first instance to the unique_data list\n",
        "    unique_data.append(first_instance.copy())\n",
        "\n",
        "  # Convert the list of DataFrames to a single DataFrame\n",
        "  full_data_unique = pd.DataFrame(pd.concat(unique_data))\n",
        "  full_data_unique = full_data_unique.T\n",
        "  fim_source_df = full_data_unique[['RatingCurveID','FimSourceID']]\n",
        "  fim_source_df = fim_source_df.drop_duplicates()\n",
        "  joined_df['FIMSourceID'] = joined_df.apply(lambda row: fim_source_df.loc[fim_source_df['RatingCurveID'] == row['RatingCurveID'], 'FimSourceID'].values[0] if row['RatingCurveID'] in fim_source_df['RatingCurveID'].values else None, axis=1)\n",
        "  joined_df = joined_df.sort_values(by=['FileID'])\n",
        "\n",
        "  return joined_df\n",
        "\n",
        "def create_filename(row):\n",
        "  \"\"\"\n",
        "  Creates a new filename based FIMSourceID, RatingCurveID, data type, and flow.\n",
        "  Example: 1-1-EXT-1000 (FIMSourceID-RatingCurveID-DataType(floodextentvector)-Flow)\n",
        "  Args:\n",
        "      row (pd.Series): A row from the DataFrame.\n",
        "  Returns:\n",
        "      str: The constructed new filename.\n",
        "  \"\"\"\n",
        "  fim_source_id = int(row['FIMSourceID']) if pd.notna(row['FIMSourceID']) else None\n",
        "  file_id = int(row['FileID']) if pd.notna(row['FileID']) else None\n",
        "  rating_curve_id = int(row['RatingCurveID']) if pd.notna(row['RatingCurveID']) else None\n",
        "  flood_extent_vector_id = int(row['FloodExtentVectorID']) if pd.notna(row['FloodExtentVectorID']) else None\n",
        "  depth_raster_id = int(row['DepthRasterID']) if pd.notna(row['DepthRasterID']) else None\n",
        "  wse_raster_id = int(row['WSERasterID']) if pd.notna(row['WSERasterID']) else None\n",
        "  vel_raster_id = int(row['VelocityRasterID']) if pd.notna(row['VelocityRasterID']) else None\n",
        "  boundary_vector_id = int(row['BoundaryVectorID']) if pd.notna(row['BoundaryVectorID']) else None\n",
        "  flow = int(row['flow']) if pd.notna(row['flow']) else None\n",
        "  filename = row['filename']\n",
        "  # Check if FileID is null and if it is don't rename the file as it is not an output file or is not in the database\n",
        "  if pd.isna(file_id):\n",
        "    return row['filename']\n",
        "  # Construct filename\n",
        "  filename_parts = [str(fim_source_id)]\n",
        "  filename_parts += [str(rating_curve_id)]\n",
        "  if flood_extent_vector_id == file_id:\n",
        "    filename_parts += ['EXT']\n",
        "  elif depth_raster_id == file_id:\n",
        "    filename_parts += ['DEP']\n",
        "  elif wse_raster_id == file_id:\n",
        "    filename_parts += ['WSE']\n",
        "  elif vel_raster_id == file_id:\n",
        "    filename_parts += ['VEL']\n",
        "  elif boundary_vector_id == file_id:\n",
        "    filename_parts += ['BND']\n",
        "  filename_parts.append(str(flow))\n",
        "  filename_full = '-'.join(filename_parts)\n",
        "  last_period_index = filename.rfind('.')\n",
        "  filename_full += filename[last_period_index:]\n",
        "\n",
        "  return filename_full\n",
        "\n",
        "processed_dataframe = process_dataframe(filenames)\n",
        "processed_dataframe['new_filename'] = processed_dataframe.apply(create_filename, axis=1)\n",
        "processed_dataframe = processed_dataframe[['filename','new_filename']]\n",
        "shp_rows = processed_dataframe[processed_dataframe['new_filename'].str.endswith('.shp')]\n",
        "# Function to create auxiliary file entry\n",
        "def create_auxiliary_entry(row, extension):\n",
        "    \"\"\"\n",
        "    Creates a Series with entries for 'filename' and 'new_filename' with modified extensions.\n",
        "    Args:\n",
        "        row: A pandas Series representing a row in the DataFrame.\n",
        "        extension: String representing the new extension (e.g., 'dbf', 'prj', 'shx').\n",
        "    Returns:\n",
        "        A pandas Series with modified filenames and new filenames.\n",
        "    \"\"\"\n",
        "    return pd.Series({\n",
        "        'filename': row['filename'].replace('.shp', f'.{extension}'),\n",
        "        'new_filename': row['new_filename'].replace('.shp', f'.{extension}')\n",
        "    })\n",
        "auxiliary_dfs = []\n",
        "for extension in ['dbf', 'prj', 'shx']:\n",
        "    auxiliary_df = shp_rows.apply(lambda row: create_auxiliary_entry(row, extension), axis=1)\n",
        "    auxiliary_dfs.append(auxiliary_df)\n",
        "\n",
        "processed_dataframe = pd.concat([processed_dataframe] + auxiliary_dfs, ignore_index=True)\n",
        "processed_dataframe.to_csv('new_filenames.csv', index=False)\n",
        "print(\"\"\"New filenames saved to 'new_filenames.csv' in the Google Colab files explorer.\n",
        "Make sure to download this file for future reference\"\"\")\n",
        "# Check if the files exist in the Colab file explorer before attempting to rename\n",
        "file_check = processed_dataframe.copy()\n",
        "file_check = processed_dataframe[processed_dataframe['filename'] != f'{db_file[:-3]}.json']\n",
        "bad_files = []\n",
        "for each in file_check['filename']:\n",
        "  if not os.path.exists(each):\n",
        "    bad_files.append(each)\n",
        "if len(bad_files) > 0:\n",
        "  for each in bad_files:\n",
        "    print(f'Error: {each} does not exist')\n",
        "  raise ValueError('Some necessary files are not in the Colab file explorer')\n",
        "# Use the new filenames to rename the files before export\n",
        "for index, row in processed_dataframe.iterrows():\n",
        "  old_name = row['filename']\n",
        "  new_name = row['new_filename']\n",
        "  #check if old file name exists in the file explorer, and if it does rename the file\n",
        "  if os.path.exists(old_name):\n",
        "    os.rename(old_name, new_name)\n",
        "def rename_files_in_database(dataframe, database_path):\n",
        "  \"\"\"\n",
        "  Renames files in an SQLite database based on a DataFrame.\n",
        "  Args:\n",
        "      dataframe (pd.DataFrame): The DataFrame containing filename and new_filename columns.\n",
        "      database_path (str): The path to the SQLite database.\n",
        "  \"\"\"\n",
        "  # Update query\n",
        "  update_query = \"\"\"UPDATE files SET filename = ? WHERE filename = ?\"\"\"\n",
        "  # Loop through DataFrame and update database\n",
        "  for index, row in dataframe.iterrows():\n",
        "    original_filename = row['filename']\n",
        "    new_filename = row['new_filename']\n",
        "    cursor.execute(update_query, (new_filename, original_filename))\n",
        "\n",
        "  # Commit changes and close connection\n",
        "  conn.commit()\n",
        "  conn.close()\n",
        "  return\n",
        "rename_files_in_database(processed_dataframe, db_file)\n",
        "\n",
        "def sqlite_to_json(db_filename, json_filename):\n",
        "  data = {}\n",
        "  conn = sqlite3.connect(db_filename)\n",
        "  conn.row_factory = sqlite3.Row  # Use row factory for dictionary-like access\n",
        "\n",
        "  cursor = conn.cursor()\n",
        "  cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
        "  tables = [row['name'] for row in cursor.fetchall()]\n",
        "\n",
        "  for table in tables:\n",
        "    cursor.execute(f\"SELECT * FROM {table}\")\n",
        "    rows = cursor.fetchall()\n",
        "    data[table] = [dict(row) for row in rows]\n",
        "\n",
        "  with open(json_filename, 'w', encoding='utf-8') as f:\n",
        "    json.dump(data, f, indent=4)\n",
        "  conn.close()\n",
        "\n",
        "json_filename = db_file[:-3] + '.json'\n",
        "sqlite_to_json(db_file, json_filename)\n",
        "print(f\"Converted SQLite database '{db_file}' to JSON file '{json_filename}'\")\n",
        "new_filenames = processed_dataframe['new_filename'].to_list()\n",
        "if resource_type == 'New':\n",
        "  new_resource = hs.create()\n",
        "  resIdentifier = new_resource.resource_id\n",
        "  print('Your new resource is available at: ' +  str(new_resource.metadata.url))\n",
        "  print('resource_id = '+ str(resIdentifier))\n",
        "  new_resource.metadata.title = resource_name\n",
        "  new_resource.metadata.abstract = resource_description\n",
        "  new_resource.metadata.subjects = resource_subjects\n",
        "  new_resource.save()\n",
        "  resource = new_resource\n",
        "else:\n",
        "  resource = hs.resource(resource_id)\n",
        "print(\"\"\"\n",
        "The code will most likely throw an error when it is done uploading the files\n",
        "if you have a lot of files. After the error appears you should check to make\n",
        "sure that your files have all uploaded by viewing your resource on HydroShare at:\n",
        "\"\"\"\n",
        " + str(resource.metadata.url))\n",
        "resource.file_upload(*new_filenames)"
      ],
      "metadata": {
        "id": "vqV7AIJEA23z",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After uploading your data to HydroShare you will need to change the resource sharing status to public in order for your files to be able to be visualized using the visualization application. You can also add additional information if desired."
      ],
      "metadata": {
        "id": "tMARjv-fHZPD"
      }
    }
  ]
}
